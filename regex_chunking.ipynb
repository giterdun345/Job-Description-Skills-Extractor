{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "trSlIMT_U8B1",
    "outputId": "20d62a4e-d905-4bd3-cd4a-edb82942e2c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "Mlb5G5izVul3",
    "outputId": "6dec2a07-2bcd-4867-b1b2-da2b1e8f8189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "id": "xxobNld2eYlU",
    "outputId": "f75bba2c-0417-47af-d3c5-ee0bee5e87e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5379 entries, 0 to 5378\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Description         5379 non-null   object\n",
      " 1   lower_description   5379 non-null   object\n",
      " 2   word_tokenized      5379 non-null   object\n",
      " 3   sentence_tokenized  5379 non-null   object\n",
      " 4   word_count          5379 non-null   int64 \n",
      " 5   sentence_count      5379 non-null   int64 \n",
      " 6   clean_words         5379 non-null   object\n",
      " 7   clean_stemmed       5379 non-null   object\n",
      " 8   clean_lemmed        5379 non-null   object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 378.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('/content/drive/My Drive/df_description_processed.csv')\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "mrZ8od8-AVFe",
    "outputId": "72ba44e3-0c83-4224-a846-1d4327cd97e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4727, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicates reduce the df by 651 observations\n",
    "df2.drop_duplicates('lower_description', inplace= True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMz98Ii7FhO2"
   },
   "source": [
    "# POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84nC3dhEePhF"
   },
   "outputs": [],
   "source": [
    "def pos_series(keyword):\n",
    "    '''categorizes after tokenizing words with POS tags'''\n",
    "    tokens = nltk.word_tokenize(keyword)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ha3ngJ7NeP4v"
   },
   "outputs": [],
   "source": [
    "# cell runs slower due to computational exhaustion; gpu not active\n",
    "pos_tagged_arrs = df2.lower_description.apply(pos_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMjUNxnReQT4"
   },
   "outputs": [],
   "source": [
    "# unloads the tuples from the tree object for easier manipulation\n",
    "pos_tagged = []\n",
    "for row in pos_tagged_arrs.values:\n",
    "    for element in row:\n",
    "        pos_tagged.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PM4QXbWNeviQ"
   },
   "outputs": [],
   "source": [
    "# dataframe contains all of the words with their corresponding pos tag;\n",
    "pos_df = pd.DataFrame(pos_tagged, columns = ('word','POS'))\n",
    "# special chars were removed due to irrelevance as a tag but will be included in regex\n",
    "char_removal = [',', '.', ':', '#', '$', '\\'\\'', '``', '(', ')']\n",
    "drop_indices = (pos_df.loc[pos_df.POS.isin(char_removal)].index)\n",
    "pos_df.drop(drop_indices, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1R-tCsqAVSqT"
   },
   "outputs": [],
   "source": [
    "# print out of the following tags for easy reference\n",
    "# for tag in pos_df.POS.unique():\n",
    "#     print(nltk.help.upenn_tagset(tag))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeEReq61HT1r"
   },
   "source": [
    "# Noun Prase #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1n4F_64VS7q"
   },
   "outputs": [],
   "source": [
    "# defining grammer within the text using reg expressions\n",
    "# optional determinate, any number of adjectives, a noun, noun plural, proper noun with additionals following\n",
    "grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "chunkParser = nltk.RegexpParser(grammar1)\n",
    "tree1 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhQm0Y6EYG8T"
   },
   "outputs": [],
   "source": [
    "# typical noun phrase pattern to be pickled for later analyses\n",
    "g1_chunks = []\n",
    "for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "    # print(subtree)\n",
    "    g1_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mqb7BCe2mr95"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/chunks_1.pickle', 'wb') as fp1:\n",
    "    pickle.dump(g1_chunks, fp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8AsEdg8Hdpo"
   },
   "source": [
    "# Noun Phrase #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7AuWWmCih7w"
   },
   "outputs": [],
   "source": [
    "# Noun phrase variation \n",
    "# preposition maybe, any number of adjective or nouns, any plural nouns or singular nouns\n",
    "grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "chunkParser = nltk.RegexpParser(grammar2)\n",
    "tree2 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZtufY7eYKwJ"
   },
   "outputs": [],
   "source": [
    "# variation of a noun phrase pattern to be pickled for later analyses\n",
    "g2_chunks = []\n",
    "for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "    # print(subtree)\n",
    "    g2_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQ8ApP1fmyod"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/chunks_2.pickle', 'wb') as fp2:\n",
    "    pickle.dump(g2_chunks , fp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUPMERSrHhiF"
   },
   "source": [
    "Verb-Noun Pattern #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLkcIxO0VTO9"
   },
   "outputs": [],
   "source": [
    "# any sort of verb followed by any number of nouns\n",
    "grammar3 = ('''\n",
    "    VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}\n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar3)\n",
    "tree3 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pR3EFTcAjN1C"
   },
   "outputs": [],
   "source": [
    "# verb-noun pattern to be pickled for later analyses\n",
    "g3_chunks = []\n",
    "for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "    # print(subtree)\n",
    "    g3_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UooPLPB0m6p_"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/chunks_3.pickle', 'wb') as fp3:\n",
    "    pickle.dump(g3_chunks, fp3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebFXJ-PQHmqh"
   },
   "source": [
    "# <noun, noun*> Pattern #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmrbtjenVTfR"
   },
   "outputs": [],
   "source": [
    "# any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "grammar4 = ('''\n",
    "    Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} \n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar4)\n",
    "tree4 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4WOwZEb8kHLl"
   },
   "outputs": [],
   "source": [
    "# common pattern of listing skills to be pickled for later analyses\n",
    "g4_chunks = []\n",
    "for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "    # print(subtree)\n",
    "    g4_chunks.append(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Krjr8UmgnYnS"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/chunks_4.pickle', 'wb') as fp4:\n",
    "    pickle.dump(g4_chunks, fp4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Job Descriptions_POS_and_Chunking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
